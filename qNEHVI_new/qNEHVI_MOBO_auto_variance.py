"""
Ï¥àÍ∏∞ Îç∞Ïù¥ÌÑ∞ÏóêÎèÑ Î∞òÎ≥µ Îç∞Ïù¥ÌÑ∞ Ìè¨Ìï®_qÎäî Î™®ÎëêÎ∞òÎ≥µÌïòÏó¨ Î∂ÑÏÇ∞Ï†ÅÏö©_
"Ïã§Ï†ÑÏù¥Î©¥ Ïù¥Í±∞ÏÇ¨Ïö©ÌïòÎùº"

Î∞òÎ≥µ Ï∏°Ï†ïÌïú ÏÉòÌîåÎì§ÏùÄ Í∞ÅÍ∞Å ÏûêÍ∏∞ Î∂ÑÏÇ∞ÏùÑ ÎÖ∏Ïù¥Ï¶àÎ°ú ÏÇ¨Ïö©Ìï®

Îã®Ïùº Ï∏°Ï†ï ÏÉòÌîåÏùò Î∂ÑÏÇ∞(ÎÖ∏Ïù¥Ï¶à)ÏùÄ Ìïú Î≤àÏù¥ÎùºÎèÑ Î∞òÎ≥µ Ï∏°Ï†ïÎêú ÏÉòÌîåÏù¥ ÏûàÏúºÎ©¥ Í∑∏Îì§Ïùò ÌèâÍ∑† Î∂ÑÏÇ∞ÏùÑ ÏÇ¨Ïö©

Î™®Îì† ÏÉòÌîåÏù¥ Îã®Ïùº Ï∏°Ï†ïÏù¥Î©¥ ‚Üí default_noise (0.01) ÏÇ¨Ïö©
--> Îã§Îßå Ïù¥Îü∞ Í≤ΩÏö∞Îäî ÎßåÎì§ÏßÄ ÎßêÍ≥†
Ïù¥ÎïåÎäî 'qNEHVI_MOBO_code_GPU_ÌïúÎ≤àÏî© ÏãúÌñâÌïòÎäî ÏΩîÎìú'Î•º Ïù¥Ïö©ÌïòÎùº Ïù¥ ÏΩîÎìúÎäî Î™®Îì† ÏÉòÌîåÏùò ÌÜµÏùºÎêú ÎÖ∏Ïù¥Ï¶à(Î∂ÑÏÇ∞ÏùÑ) ÌïôÏäµÏãúÌÇ®Îã§

==========================================================
   qNEHVI-based Multi-objective Bayesian Optimization (auto variance detection)
   ‚úÖ y_dataÎßå ÏûÖÎ†• (3n Í∏∏Ïù¥ ‚Üí nÌöå Ï∏°Ï†ï)
   ‚úÖ Í∞Å ÏÉòÌîåÎ≥Ñ Ï∏°Ï†ï ÌöüÏàò ÏûêÎèô Ïù∏Ïãù ÌõÑ ÌèâÍ∑†¬∑Î∂ÑÏÇ∞ Í≥ÑÏÇ∞
   ‚úÖ FixedNoiseGP Í∏∞Î∞ò Î™®Îç∏
   ‚úÖ Hypervolume Í≥ÑÏÇ∞ Î∞è Ïù∏ÏáÑ
==========================================================
"""

import os
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"

import torch
import numpy as np
import pandas as pd
from torch.optim import Adam

# ==========================================================
# FIXEDNOISEGP IMPORT (BoTorch Î≤ÑÏ†ÑÎ≥Ñ ÎåÄÏùë)
# ==========================================================
try:
    from botorch.models import FixedNoiseGP  # 0.16 Ïù¥ÏÉÅ
except ImportError:
    try:
        from botorch.models.gp_regression_fixed import FixedNoiseGP  # 0.15.0~0.15.1
    except ImportError:
        from botorch.models.gp_regression import SingleTaskGP
        from gpytorch.likelihoods import FixedNoiseGaussianLikelihood
        class FixedNoiseGP(SingleTaskGP):
            """Fallback: older BoTorch builds"""
            def __init__(self, train_X, train_Y, train_Yvar, **kwargs):
                likelihood = FixedNoiseGaussianLikelihood(noise=train_Yvar)
                super().__init__(train_X, train_Y, likelihood=likelihood, **kwargs)

from gpytorch.kernels import ScaleKernel, MaternKernel
from botorch.models.transforms import Normalize, Standardize
from botorch.acquisition.multi_objective.logei import qLogNoisyExpectedHypervolumeImprovement
from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective
from botorch.utils.multi_objective.box_decompositions import NondominatedPartitioning
from botorch.utils.multi_objective.hypervolume import Hypervolume
from botorch.utils.multi_objective.pareto import is_non_dominated
from botorch.sampling.normal import SobolQMCNormalSampler
from botorch.optim.initializers import gen_batch_initial_conditions
from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood

# ==========================================================
# CONFIGURATION
# ==========================================================
config = {
    "num_objectives": 3,
    "num_variables": 12,
    "maximize": [True, True, True],
    "bounds": [
    (-8.219935292911435, 4.8678405666098925),
    (-4.529524136731765, 8.491828249116784),
    (-4.485586547153938, 7.464671202787634),
    (-4.678683041062727, 4.10771660957421),
    (-3.1486780009041593, 4.743911725110442),
    (-3.8200012211312333, 4.238666876454304),
    (-3.6859282450412456, 3.6804077253203857),
    (-3.8340695577663375, 3.600410804950003),
    (-3.7403453350118108, 3.2409026104937992),
    (-2.6439237960579014, 3.584396130267221),
    (-2.6281305042188903, 2.895901685121338),
    (-3.5432124601001793, 2.4775653447131045)
    ],
    "batch_q": 4,
    "num_restarts": 10,
    "raw_samples": 512,
    "random_seed": 42,
    "default_noise": 0.0001
}

torch.manual_seed(config["random_seed"])
dtype = torch.double
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {device}, dtype: {dtype}")


# ==========================================================
# AUTO-DETECT REPETITIONS, COMPUTE MEAN & VAR
# ==========================================================
# 1Ï∞® Î£®ÌîÑ: Î∞òÎ≥µ ÏÉòÌîåÎì§Ïùò Î∂ÑÏÇ∞ Î®ºÏ†Ä ÏàòÏßë
Y_mean_list, Y_var_list, all_repeated_vars = [], [], []

for i, ylist in enumerate(y_data):
    y = np.array(ylist)
    n_meas = len(y) // config["num_objectives"]
    y = y.reshape(n_meas, config["num_objectives"])

    if n_meas > 1:
        var = np.var(y, axis=0, ddof=1)
        all_repeated_vars.append(var)

# üî∏ Î∞òÎ≥µ Ï∏°Ï†ï Î∂ÑÏÇ∞Îì§Ïùò Ï†ÑÏ≤¥ ÌèâÍ∑†
if len(all_repeated_vars) > 0:
    global_mean_var = np.mean(np.vstack(all_repeated_vars), axis=0)
    print(f"üåê Global mean variance from repeated samples: {global_mean_var}")
else:
    global_mean_var = np.ones(config["num_objectives"]) * config["default_noise"]
    print(f"‚ö™ No repeated samples ‚Üí using default noise: {global_mean_var}")

# 2Ï∞® Î£®ÌîÑ: ÌèâÍ∑†/Î∂ÑÏÇ∞ ÏµúÏ¢Ö Í≥ÑÏÇ∞
for i, ylist in enumerate(y_data):
    y = np.array(ylist)
    n_meas = len(y) // config["num_objectives"]
    y = y.reshape(n_meas, config["num_objectives"])

    if n_meas == 1:
        mean = y[0]
        var = global_mean_var  # ‚úÖ Ïã§Ï†ú Î∞òÎ≥µ Ï∏°Ï†ï ÌèâÍ∑† Î∂ÑÏÇ∞ÏúºÎ°ú ÎåÄÏ≤¥
        print(f"‚ö™ Sample {i}: 1 measurement ‚Üí global mean variance applied")
    else:
        mean = np.mean(y, axis=0)
        var = np.var(y, axis=0, ddof=1)
        print(f"üîµ Sample {i}: {n_meas} measurements ‚Üí variance computed")

    Y_mean_list.append(mean)
    Y_var_list.append(np.maximum(var, 1e-6))

X = torch.tensor(x_data, dtype=dtype, device=device)
Y = torch.tensor(np.array(Y_mean_list), dtype=dtype, device=device)
Yvar = torch.tensor(np.array(Y_var_list), dtype=dtype, device=device)

for i, maximize in enumerate(config["maximize"]):
    if not maximize:
        Y[:, i] = -Y[:, i]

print(f"‚úÖ Data loaded: X={X.shape}, Y={Y.shape}, Yvar={Yvar.shape}")

# ==========================================================
# MODEL
# ==========================================================
model = FixedNoiseGP(
    train_X=X,
    train_Y=Y,
    train_Yvar=Yvar,
    covar_module=ScaleKernel(MaternKernel(nu=2.5, ard_num_dims=config["num_variables"])),
    input_transform=Normalize(config["num_variables"]),
    outcome_transform=Standardize(config["num_objectives"]),
).to(device=device, dtype=dtype)

mll = ExactMarginalLogLikelihood(model.likelihood, model)
opt = Adam(mll.parameters(), lr=0.01)
for _ in range(200):
    opt.zero_grad()
    output = model(*model.train_inputs)
    loss = -mll(output, model.train_targets).sum()
    loss.backward()
    opt.step()
print("‚úÖ GP MLE optimization finished.")

# ==========================================================
# REFERENCE POINT
# ==========================================================
with torch.no_grad():
    model.eval()
    mean = model.posterior(X).mean
    Y_adj = mean.clone()
    for i, maximize in enumerate(config["maximize"]):
        if not maximize:
            Y_adj[:, i] = -Y_adj[:, i]
    ref_point = (Y_adj.min(0).values - 2.0).clamp(min=-10.0)
    ref_point = torch.where(torch.isnan(ref_point),
                            torch.tensor(-1.0, dtype=dtype, device=device),
                            ref_point)
print(f"üìç Safe ref_point: {ref_point.tolist()}")

# ==========================================================
# HYPERVOLUME CALCULATION
# ==========================================================
with torch.no_grad():
    mask = is_non_dominated(Y)
    pareto_front = Y[mask]
    hv = Hypervolume(ref_point=ref_point).compute(pareto_front)
    print(f"üåà Hypervolume (current data): {float(hv):.4f}")

# ==========================================================
# ACQUISITION FUNCTION
# ==========================================================
partitioning = NondominatedPartitioning(ref_point=ref_point, Y=Y_adj)
sampler = SobolQMCNormalSampler(sample_shape=torch.Size([64]))
acq_func = qLogNoisyExpectedHypervolumeImprovement(
    model=model,
    ref_point=ref_point.tolist(),
    X_baseline=X,
    objective=IdentityMCMultiOutputObjective(),
    prune_baseline=True,
    sampler=sampler,
)

# ==========================================================
# ACQUISITION OPTIMIZATION
# ==========================================================
def optimize_acqf_torch(acqf, bounds, q, num_restarts, raw_samples, steps=100):
    bounds_t = torch.tensor(bounds, dtype=dtype, device=device).T
    Xraw = gen_batch_initial_conditions(acq_function=acqf, bounds=bounds_t, q=q,
                                        num_restarts=num_restarts, raw_samples=raw_samples).to(device)
    Xraw.requires_grad_(True)
    opt = Adam([Xraw], lr=0.05)
    for _ in range(steps):
        opt.zero_grad()
        loss = -acqf(Xraw).sum()
        loss.backward()
        opt.step()
        Xraw.data.clamp_(bounds_t[0], bounds_t[1])
    with torch.no_grad():
        vals = acqf(Xraw)
        best = torch.argmax(vals)
        return Xraw[best].detach(), vals[best].detach()

candidate, acq_value = optimize_acqf_torch(
    acq_func, config["bounds"], q=config["batch_q"],
    num_restarts=config["num_restarts"], raw_samples=config["raw_samples"], steps=150,
)

# ==========================================================
# OUTPUT
# ==========================================================
df_out = pd.DataFrame(candidate.cpu().numpy(),
                      columns=[f"x{i+1}" for i in range(config["num_variables"])])
print("\n===== Next Suggested Candidates =====")
print(df_out.to_string(index=False))
print(f"\nüìà Acquisition Value: {acq_value.item():.6f}")
